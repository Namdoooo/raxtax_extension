# Benchmark: Reference Count vs Runtime

## Purpose
This benchmark evaluates how the runtime performance of the extended
raxtax workflow scales with an increasing number of reference sequences.
The objective is to assess the impact of reference count on execution time.

## Benchmark Design
The benchmark is organized into multiple independent iterations.
Within each iteration, the number of references is varied between 400 and
3200 while all other parameters remain fixed.

For each test within an iteration, a new dataset is generated using
different random seeds. In total, five iterations are performed.
Execution time is measured to evaluate runtime performance.

## Data Generation
All input data used in this benchmark is generated automatically by the
benchmark scripts. No external input data is required.

For each test, a new dataset is generated using random seeds. Both the
query sequences and the corresponding reference data are regenerated
for each reference count.

## Execution workflow
The benchmark execution is organized in multiple layers.

The `main.py` file in the benchmark root directory orchestrates the
execution of all iterations. For each iteration, it invokes the
corresponding `main.py` located in the iteration subdirectory.

The `main.py` file inside each iteration directory is identical across
all iterations and is responsible for executing all tests within that
iteration. For each iteration, the required directory structure for the
individual tests is created automatically.

For each test, the iteration-level `main.py` performs the following
steps:

1. Generates a configuration file and an executable to run the
   simulation.
2. Executes the simulation using the generated executable and
   configuration file.
3. Generates the input data based on the configuration file.
4. Executes the classification algorithm (raxtax extension) on the
   generated input data.

## How to Run
From the raxtax root directory, execute

```
python -m benchmarks_hits.reference_count_benchmark.main
```

After completion, analysis and plotting can be performed with:

```
python -m benchmarks_hits.reference_count_benchmark.analyze
```

## Output

### Per test (within each iteration)
For each test run, the following directories and files are generated:

- `queries/`  
  Contains the generated query sequences used as input for the
  classification algorithm. This directory also includes intermediate
  files and log files produced during query generation.
- `references/`  
  Contains the generated reference data used for classification. This
  directory also includes intermediate files, log files, and a lookup
  table required by the classification algorithm.
- `results_*/`  
  Contains the classification results and a `metadata.out` file containing
  runtime measurements and classification statistics.

For archival purposes, the `queries/` and `references/` directories
generated by the benchmark were manually zipped together as
`dataset.tgz` to reduce storage requirements.

### After analysis
After executing the analysis script (`analyze.py`), the following
additional output files are generated:

- `aggregated_metadata.csv`  
  Aggregated metadata for all tests within a single iteration.
- `combined_metadata.csv`  
  Aggregated metadata across all iterations.
- `plots/`  
  Directory containing generated plots.
- `leaf_count_vs_calculate_intersection_sizes_time.pdf`  
  Final plot showing the relationship between reference count and runtime.

## Notes
The benchmark was executed using commit `ac106c8f4283941f6cd036ebb6ba27347701be04` of the `raxtax_extension`
repository.