# Benchmark: Thread Scalability

## Purpose
This benchmark evaluates how the runtime performance of the extended
raxtax workflow scales with an increasing number of threads. The
objective is to measure the speedup achieved through parallel execution
and to assess the scalability of the method with respect to the number
of available CPU cores.

## Benchmark Design
The benchmark is organized into multiple independent iterations.
Within each iteration, the number of threads (core count) is varied
between predefined values while all other parameters remain fixed.
Within a single iteration, the same input data (reference and query
sequences) are used for all thread counts. Between iterations, new input
data are generated using different random seeds. A total of 5
iterations are performed.

## Data Generation
All input data used in this benchmark is generated automatically by 
the benchmark scripts. No external input data is required.

For each iteration, a new dataset is generated using random seeds. The
same dataset is reused for all thread counts within that iteration.

## Execution workflow
The benchmark execution is organized in multiple layers.

The `main.py` file in the benchmark root directory orchestrates the
execution of all iterations. For each iteration, it invokes the
corresponding `main.py` located in the iteration subdirectory.

The `main.py` file inside each iteration directory is identical across
all iterations and is responsible for executing all tests within that
iteration. For each iteration, the required directory structure for the
individual tests is created automatically.

For each test, the iteration-level `main.py` performs the following
steps:

1. Generates a configuration file and an executable to run the
   simulation with the specified average branch length.
2. Executes the simulation using the generated executable and
   configuration file.
3. Generates the input data based on the configuration file.
4. Executes the classification algorithm (raxtax extension) on the
   generated input data.

## How to Run
From the raxtax root directory, execute

```
python -m benchmarks_hits.core_count_benchmark.main
```

After completion, analysis and plotting can be performed with:

```
python -m benchmarks_hits.core_count_benchmark.analyze
```

## Output

### Per test (within each iteration)
For each test run, the following directories and files are generated:

- `queries/`  
  Contains the generated query sequences used as input for the
  classification algorithm. This directory also includes intermediate
  files and log files produced during query generation.
- `references/`  
  Contains the generated reference data used for classification. This
  directory also includes intermediate files, log files, and a lookup
  table required by the classification algorithm.
- `results_*/`  
  Contains the classification results and a `metadata.out` file containing
  runtime measurements and classification statistics.

For archival purposes, the `queries/` and `references/` directories
generated by the benchmark were manually zipped together as
`dataset.tgz` to reduce storage requirements.

### After analysis
After executing the analysis script (`analyze.py`), the following
additional output files are generated:

- `aggregated_metadata.csv`  
  Aggregated metadata for all tests within a single iteration.
- `combined_metadata.csv`  
  Aggregated metadata across all iterations.
- `plots/`  
  Directory containing generated plots.
- `threads_vs_rel_speedup.pdf`  
  Final plot showing the relationship between core count and speedup.

## Notes
The benchmark was executed using commit `669faf03004cba325b5befac602f0d6758ff6bd2` of the `raxtax_extension`
repository.